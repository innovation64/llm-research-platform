(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[717],{2149:function(e,s,i){(window.__NEXT_P=window.__NEXT_P||[]).push(["/phd",function(){return i(1395)}])},1395:function(e,s,i){"use strict";i.r(s),i.d(s,{default:function(){return t}});var n=i(5893),a=i(1664),r=i.n(a),l=i(6250);function t(){return(0,n.jsxs)("div",{children:[(0,n.jsx)("section",{className:"bg-gradient-to-r from-blue-600 to-blue-800 text-white py-16",children:(0,n.jsx)("div",{className:"container mx-auto px-4",children:(0,n.jsxs)("div",{className:"max-w-3xl mx-auto text-center",children:[(0,n.jsx)("h1",{className:"text-4xl md:text-5xl font-bold mb-6",children:"PhD Position in LLM Research"}),(0,n.jsx)("p",{className:"text-xl md:text-2xl mb-4 opacity-90",children:"Looking for opportunities to advance research in multi-agent systems and RAG frameworks"})]})})}),(0,n.jsx)("section",{className:"section bg-white",children:(0,n.jsx)("div",{className:"container",children:(0,n.jsxs)("div",{className:"max-w-4xl mx-auto",children:[(0,n.jsxs)("div",{className:"bg-blue-50 rounded-lg p-6 mb-10",children:[(0,n.jsx)("h2",{className:"text-2xl font-semibold mb-4",children:"Overview"}),(0,n.jsx)("p",{className:"text-lg mb-6",children:"I am actively seeking PhD positions to continue my research in large language models, multi-agent systems, and RAG frameworks. With experience at Tsinghua University NLP Lab and HKMU, I'm looking to join a research team that focuses on advancing the capabilities of language models."}),(0,n.jsxs)("div",{className:"grid grid-cols-1 md:grid-cols-2 gap-6",children:[(0,n.jsxs)("div",{className:"flex",children:[(0,n.jsx)("div",{className:"flex-shrink-0 mr-4 text-primary",children:(0,n.jsx)(l.HR2,{size:24})}),(0,n.jsxs)("div",{children:[(0,n.jsx)("h3",{className:"font-medium text-lg mb-1",children:"Academic Background"}),(0,n.jsx)("p",{children:"Bachelor's in Information Management and Systems from Beijing University of Chinese Medicine (2018-2022)"})]})]}),(0,n.jsxs)("div",{className:"flex",children:[(0,n.jsx)("div",{className:"flex-shrink-0 mr-4 text-primary",children:(0,n.jsx)(l.hbr,{size:24})}),(0,n.jsxs)("div",{children:[(0,n.jsx)("h3",{className:"font-medium text-lg mb-1",children:"Research Experience"}),(0,n.jsx)("p",{children:"HKMU (2023-Present), Tsinghua University NLP Lab (2023), Zhipu AI (2022-2023)"})]})]}),(0,n.jsxs)("div",{className:"flex",children:[(0,n.jsx)("div",{className:"flex-shrink-0 mr-4 text-primary",children:(0,n.jsx)(l.xUG,{size:24})}),(0,n.jsxs)("div",{children:[(0,n.jsx)("h3",{className:"font-medium text-lg mb-1",children:"Research Focus"}),(0,n.jsx)("p",{children:"Multi-agent systems, RAG, reasoning, and long-term memory in LLMs"})]})]}),(0,n.jsxs)("div",{className:"flex",children:[(0,n.jsx)("div",{className:"flex-shrink-0 mr-4 text-primary",children:(0,n.jsx)(l.WY8,{size:24})}),(0,n.jsxs)("div",{children:[(0,n.jsx)("h3",{className:"font-medium text-lg mb-1",children:"Community Involvement"}),(0,n.jsx)("p",{children:"Hugging Face Chinese Community, AITIME, Academic Journal Reviewer"})]})]})]})]}),(0,n.jsx)("h2",{className:"text-2xl font-semibold mb-6",children:"Research Interests for PhD"}),(0,n.jsxs)("div",{className:"space-y-6 mb-10",children:[(0,n.jsxs)("div",{className:"border-l-4 border-primary pl-4",children:[(0,n.jsx)("h3",{className:"font-medium text-lg mb-2",children:"Long-term Memory Mechanisms in LLMs"}),(0,n.jsx)("p",{children:"Investigating architectures that enable LLMs to maintain and effectively utilize information over extended interactions, beyond current context window limitations."})]}),(0,n.jsxs)("div",{className:"border-l-4 border-primary pl-4",children:[(0,n.jsx)("h3",{className:"font-medium text-lg mb-2",children:"Multi-Agent Evaluation Frameworks"}),(0,n.jsx)("p",{children:"Developing robust methodologies to evaluate collaboration, specialization, and information sharing between multiple LLM-based agents in complex task environments."})]}),(0,n.jsxs)("div",{className:"border-l-4 border-primary pl-4",children:[(0,n.jsx)("h3",{className:"font-medium text-lg mb-2",children:"Context-Aware Reasoning for Ambiguous Tasks"}),(0,n.jsx)("p",{children:"Building on my CondAmbigQA research to enable LLMs to handle ambiguity through contextual understanding and multi-step reasoning processes."})]}),(0,n.jsxs)("div",{className:"border-l-4 border-primary pl-4",children:[(0,n.jsx)("h3",{className:"font-medium text-lg mb-2",children:"RAG Architecture for Specialized Domains"}),(0,n.jsx)("p",{children:"Exploring retrieval-augmented generation techniques optimized for domain-specific knowledge integration, focusing on medical and scientific applications."})]})]}),(0,n.jsx)("h2",{className:"text-2xl font-semibold mb-6",children:"Publications"}),(0,n.jsxs)("div",{className:"space-y-6 mb-10",children:[(0,n.jsxs)("div",{className:"p-4 border rounded-lg",children:[(0,n.jsx)("h3",{className:"font-medium text-lg",children:"CondAmbigQA: A Benchmark and Dataset for Conditional Ambiguous Question Answering"}),(0,n.jsx)("p",{className:"text-gray-600 mb-2",children:"Li, Yang, et al. (2025). arXiv preprint arXiv:2502.01523"}),(0,n.jsxs)("div",{className:"flex gap-4",children:[(0,n.jsx)("a",{href:"#",className:"text-primary hover:underline",children:"Paper"}),(0,n.jsx)("a",{href:"#",className:"text-primary hover:underline",children:"Code"}),(0,n.jsx)("a",{href:"#",className:"text-primary hover:underline",children:"Dataset"})]})]}),(0,n.jsxs)("div",{className:"p-4 border rounded-lg",children:[(0,n.jsx)("h3",{className:"font-medium text-lg",children:"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X"}),(0,n.jsx)("p",{className:"text-gray-600 mb-2",children:"Zheng, Qinkai, et al. (2023). KDD 2023"}),(0,n.jsxs)("div",{className:"flex gap-4",children:[(0,n.jsx)("a",{href:"#",className:"text-primary hover:underline",children:"Paper"}),(0,n.jsx)("a",{href:"#",className:"text-primary hover:underline",children:"Code"})]})]}),(0,n.jsxs)("div",{className:"p-4 border rounded-lg",children:[(0,n.jsx)("h3",{className:"font-medium text-lg",children:"Machine Learning Models for Stroke Detection by Observing Eye-Movement Features Under Five-Color Visual Stimuli in Traditional Chinese Medicine"}),(0,n.jsx)("p",{className:"text-gray-600 mb-2",children:"Lu, Qingya, et al. (2023). Journal of Traditional Chinese Medical Sciences"}),(0,n.jsx)("div",{className:"flex gap-4",children:(0,n.jsx)("a",{href:"#",className:"text-primary hover:underline",children:"Paper"})})]})]}),(0,n.jsx)("h2",{className:"text-2xl font-semibold mb-6",children:"Contact for PhD Opportunities"}),(0,n.jsxs)("div",{className:"bg-gray-50 rounded-lg p-6",children:[(0,n.jsx)("p",{className:"mb-4",children:"If you are a professor or research lab looking for PhD students in the field of LLMs, multi-agent systems, or RAG, I would be happy to discuss potential research directions and opportunities."}),(0,n.jsxs)("div",{className:"space-y-2 mb-6",children:[(0,n.jsxs)("p",{children:[(0,n.jsx)("strong",{children:"Email:"})," innovation64feng@gmail.com"]}),(0,n.jsxs)("p",{children:[(0,n.jsx)("strong",{children:"Phone:"})," +86 13269183099 / +852 54614337"]}),(0,n.jsxs)("p",{children:[(0,n.jsx)("strong",{children:"Current Institution:"})," Hong Kong Metropolitan University"]})]}),(0,n.jsxs)("div",{className:"flex flex-wrap gap-4",children:[(0,n.jsx)("a",{href:"https://innovation64.github.io/",target:"_blank",rel:"noopener noreferrer",className:"btn btn-primary",children:"View Personal Website"}),(0,n.jsx)("a",{href:"mailto:innovation64feng@gmail.com",className:"btn btn-outline",children:"Email Me"}),(0,n.jsx)(r(),{href:"/research",className:"btn bg-gray-200 text-gray-700 hover:bg-gray-300",children:"View Research"})]})]})]})})})]})}}},function(e){e.O(0,[888,774,179],function(){return e(e.s=2149)}),_N_E=e.O()}]);